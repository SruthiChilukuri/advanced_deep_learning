{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TFX using chicago taxi example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "**Chicago Taxi example using TensorFlow Extended (TFX)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SQA7Q5nej3"
      },
      "source": [
        "\r\n",
        "!pip install -q -U --use-feature=2020-resolver tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "import tfx\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "from typing import List, Text\n",
        "import os\n",
        "import absl\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "import taxi_constants\n",
        "import tensorflow_model_analysis as tfma\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ4K18_DN2D8"
      },
      "source": [
        "# Check the library versions\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufJKQ6OvkJlY"
      },
      "source": [
        "**Pipeline path setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad5JLpKbf6sN"
      },
      "source": [
        "# Below is the root directory for TFX pip package installation.\n",
        "_tfx_root = tfx.__path__[0]\n",
        "\n",
        "# Directory containing the TFX Chicago Taxi Pipeline example.\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "\n",
        "# Model serving path.\n",
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/taxi_simple')\n",
        "\n",
        "# Set up logging.\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "**Download Taxi Trips dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywX6OUEhAqn"
      },
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "dataset = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(dataset, _data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YPeLPFOXaD"
      },
      "source": [
        "# Check the dataset\r\n",
        "!head {_data_filepath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "**Interactive Context Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rh6K5sUf9dd"
      },
      "source": [
        "# Create InteractiveContext allowing to run TFX components interactively.\n",
        "interactive_context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "**ExampleGen**\n",
        "\n",
        "The `ExampleGen` component will perform following activities:\n",
        "\n",
        "1.   Split data into training and evaluation sets (by default, 2/3 training + 1/3 eval)\n",
        "2.   Convert data into the `tf.Example` format\n",
        "3.   Copy data into the `_tfx_root` directory for other components to access\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u"
      },
      "source": [
        "example_gen = CsvExampleGen(input=external_input(_data_root))\n",
        "interactive_context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880KkTAkPeUg"
      },
      "source": [
        "# Check examplegen output artifacts containing training and evaluation examples\n",
        "\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XIXjiCPwzQ"
      },
      "source": [
        "# URI of the output artifact representing the training examples\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
        "\n",
        "tfrecord_filename = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset`\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filename, compression_type=\"GZIP\")\n",
        "\n",
        "# Show first three records and decode them\n",
        "for i in dataset.take(3):\n",
        "  serialized_example = i.numpy()\n",
        "  record = tf.train.Example()\n",
        "  record.ParseFromString(serialized_example)\n",
        "  pp.pprint(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "**Data Analysis with StatisticsGen** </br>\n",
        "The StatisticsGen component computes statistics over the dataset for data analysis and for downstream components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "source": [
        "statistic_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "context.run(statistic_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLjXy7K6Tp_G"
      },
      "source": [
        "# Visualization of output statistics\r\n",
        "context.show(statistic_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "**SchemaGen**\n",
        "\n",
        "The SchemaGen component generates a schema based on data statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "source": [
        "schemagen = SchemaGen(\n",
        "    statistics=statistic_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schemagen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "source": [
        "# Visualization of generated schema in table format\r\n",
        "context.show(schemagen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "**ExampleValidator** </br>\n",
        "Based on input statistics and Schema, the ExampleValidator component detects anomalies present in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "source": [
        "examples_validator = ExampleValidator( statistics=statistics_gen.outputs['statistics'], schema=schema_gen.outputs['schema'])\n",
        "context.run(examples_validator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "source": [
        "# Visualization of anamolies\r\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36"
      },
      "source": [
        "**Transform** </br>\n",
        "Based on input data from ExampleGen, schema from Schemagen and module containing user-defined transform code, the Transform component performs feature engineering for both training and serving. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNSiUKb4YJf"
      },
      "source": [
        "_taxi_constant_module_file = 'taxi_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "source": [
        "%%writefile {_taxi_constant_module_file}\n",
        "\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\n",
        "MAX_CATEGORICAL_FEATURE_VALUE = [24, 31, 12]\n",
        "\n",
        "CATEGORICAL_FEATURE_KEY = ['trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
        "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
        "    'dropoff_community_area']\n",
        "\n",
        "DENSE_FLOAT_FEATURE_KEY = ['trip_miles', 'fare', 'trip_seconds']\n",
        "\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "BUCKET_FEATURE_KEY = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
        "\n",
        "# No. of vocabulary term used for encoding VOCAB_FEATURE by tf.transform\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# No. of out-of-vocab buckets\n",
        "OOV_SIZE = 10\n",
        "VOCAB_FEATURE_KEY = ['payment_type','company',]\n",
        "LABEL_KEY = 'tips'\n",
        "FARE_KEY = 'fare'\n",
        "\n",
        "def transformed_name(key):\n",
        "  return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "source": [
        "_taxi_transform_module_file = 'taxi_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "source": [
        "# \n",
        "%%writefile {_taxi_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEY = taxi_constants.DENSE_FLOAT_FEATURE_KEY\n",
        "_VOCAB_FEATURE_KEY = taxi_constants.VOCAB_FEATURE_KEY\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEY = taxi_constants.BUCKET_FEATURE_KEY\n",
        "_CATEGORICAL_FEATURE_KEY = taxi_constants.CATEGORICAL_FEATURE_KEY\n",
        "_FARE_KEY = taxi_constants.FARE_KEY\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "_transformed_name = taxi_constants.transformed_name\n",
        "\n",
        "\n",
        "def preprocessing(inputs):\n",
        "# This function returns a map from string feature key to transformed feature operations.\n",
        "  output = {}\n",
        "  for key in _DENSE_FLOAT_FEATURE_KEY:\n",
        "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
        "    output[_transformed_name(key)] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]))\n",
        "\n",
        "  for key in _VOCAB_FEATURE_KEY:\n",
        "    # Build a vocabulary for this feature.\n",
        "    output[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "        _fill_in_missing(inputs[key]),\n",
        "        top_k=_VOCAB_SIZE,\n",
        "        num_oov_buckets=_OOV_SIZE)\n",
        "\n",
        "  for key in _BUCKET_FEATURE_KEY:\n",
        "    output[_transformed_name(key)] = tft.bucketize(\n",
        "        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n",
        "\n",
        "  for key in _CATEGORICAL_FEATURE_KEY:\n",
        "    output[_transformed_name(key)] = _fill_in_missing(inputs[key])\n",
        "\n",
        "  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n",
        "  tips = _fill_in_missing(inputs[_LABEL_KEY])\n",
        "  output[_transformed_name(_LABEL_KEY)] = tf.where(\n",
        "      tf.math.is_nan(taxi_fare),\n",
        "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
        "      # Check whether the tip was > 20% of the fare\n",
        "      tf.cast(tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
        "  return output\n",
        "\n",
        "def _fill_in_missing_values(x):\n",
        "# This function replaces missing values in a SparseTensor.\n",
        "  default_values = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(tf.sparse.to_dense(tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),default_values), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "Now, we pass in this feature engineering code to the `Transform` component and run it to transform your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "source": [
        "# Transform data\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_taxi_transform_module_file))\n",
        "context.run(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SClrAaEGR1O5"
      },
      "source": [
        "# Check the transform graphs and transformed examples generated by Trasnform module.\r\n",
        "transform.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "source": [
        "# Check transform output directories\n",
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbW2zPKR_S4"
      },
      "source": [
        "# URI of the output artifact\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'train')\n",
        "# TFRecord files\n",
        "tfrecord_filename = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
        "# Create TFRecord Dataset\n",
        "tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_filename, compression_type=\"GZIP\")\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tf_record in tfrecord_dataset.take(3):\n",
        "  serialized_example = tf_record.numpy()\n",
        "  record = tf.train.Example()\n",
        "  record.ParseFromString(serialized_example)\n",
        "  pp.pprint(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJFtnl6lCg9"
      },
      "source": [
        "**Trainer** </br>\n",
        "The `Trainer` component trains a model that you define in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1376oq04YJt"
      },
      "source": [
        "_taxi_trainer_module_file = 'taxi_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf9UuNng4YJu"
      },
      "source": [
        "%%writefile {_taxi_trainer_module_file}\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEY = taxi_constants.DENSE_FLOAT_FEATURE_KEY\n",
        "_VOCAB_FEATURE_KEY = taxi_constants.VOCAB_FEATURE_KEY\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEY = taxi_constants.BUCKET_FEATURE_KEY\n",
        "_CATEGORICAL_FEATURE_KEY = taxi_constants.CATEGORICAL_FEATURE_KEY\n",
        "_max_categorical_feature_value = taxi_constants.max_categorical_feature_value\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "_transformed_name = taxi_constants.transformed_name\n",
        "\n",
        "def _transformed_name(keys):\n",
        "  return [_transformed_name(k) for k in keys]\n",
        "\n",
        "def _get_serve_tf_example(model, tf_transform_output):\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "  @tf.function\n",
        "  def serve_tf_examples(serialized_tf_example):\n",
        "    feature_specs = tf_transform_output.raw_feature_spec()\n",
        "    feature_specs.pop(_LABEL_KEY)\n",
        "    parsed_feature = tf.io.parse_example(serialized_tf_example, feature_specs)\n",
        "    transformed_feature = model.tft_layer(parsed_feature)\n",
        "    return model(transformed_feature)\n",
        "  return serve_tf_examples\n",
        "\n",
        "def _input_fn(file_pattern: List[Text], data_accessor: DataAccessor, tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  return data_accessor.tf_dataset_factory(file_pattern, dataset_options.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "\n",
        "def _build_keras_model(hidden_units: List[int] = None) -> tf.keras.Model:\n",
        "  # Creates a DNN Keras model for taxi data classification.\n",
        "  real_valued_column = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEY)\n",
        "  ]\n",
        "  categorical_column = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
        "      for key in _transformed_names(_VOCAB_FEATURE_KEY)\n",
        "  ]\n",
        "  categorical_column += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in _transformed_names(_BUCKET_FEATURE_KEY)\n",
        "  ]\n",
        "  categorical_column += [\n",
        "      tf.feature_column.categorical_column_with_identity( \n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              _transformed_names(_CATEGORICAL_FEATURE_KEY),\n",
        "              _max_categorical_feature_value)\n",
        "  ]\n",
        "  indicator_column = [\n",
        "      tf.feature_column.indicator_column(categorical_column)\n",
        "      for categorical_column in categorical_columns\n",
        "  ]\n",
        "\n",
        "  model_1 = _wide_and_deep_classifier(\n",
        "      wide_columns=indicator_column,\n",
        "      deep_columns=real_valued_column,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25])\n",
        "  return model_1\n",
        "\n",
        "\n",
        "def _wide_and_deep_classifier(wide_column, deep_column, dnn_hidden_unit):\n",
        "  # Builds a simple keras wide and deep model.\n",
        "  input_layer = {\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "      for colname in _transformed_name(_DENSE_FLOAT_FEATURE_KEY)\n",
        "  }\n",
        "  input_layer.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_name(_VOCAB_FEATURE_KEY)\n",
        "  })\n",
        "  input_layer.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_name(_BUCKET_FEATURE_KEY)\n",
        "  })\n",
        "  input_layer.update({\n",
        "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "      for colname in _transformed_name(_CATEGORICAL_FEATURE_KEY)\n",
        "  })\n",
        "\n",
        "  deep = tf.keras.layers.DenseFeatures(deep_column)(input_layer)\n",
        "  for num_nodes in dnn_hidden_units:\n",
        "    deep = tf.keras.layers.Dense(num_nodes)(deep)\n",
        "  wide = tf.keras.layers.DenseFeatures(wide_column)(input_layer)\n",
        "\n",
        "  output = tf.keras.layers.Dense(\n",
        "      1, activation='sigmoid')(\n",
        "          tf.keras.layers.concatenate([deep, wide]))\n",
        "\n",
        "  model_1 = tf.keras.Model(input_layer, output)\n",
        "  model_1.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "  model_1.summary(print_fn=absl.logging.info)\n",
        "  return model_1\n",
        "\n",
        "def run_fn(fn_arg: TrainerFnArgs):\n",
        "  # Trains the model\n",
        "  # Define parameters\n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layer = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_arg.transform_output)\n",
        "\n",
        "  train_dataset = _input_fn(fn_arg.train_files, fn_arg.data_accessor, \n",
        "                            tf_transform_output, 40)\n",
        "  eval_dataset = _input_fn(fn_arg.eval_files, fn_arg.data_accessor, \n",
        "                           tf_transform_output, 40)\n",
        "\n",
        "  model = _build_keras_model(\n",
        "      hidden_unit=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layer)\n",
        "      ])\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_arg.model_run_dir, update_freq='batch')\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_arg.train_step,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_step=fn_arg.eval_step,\n",
        "      callback=[tensorboard_callback])\n",
        "\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_serve_tf_examples_fn(model,\n",
        "                                    tf_transform_output).get_concrete_function(\n",
        "                                        tf.TensorSpec(\n",
        "                                            shape=[None],\n",
        "                                            dtype=tf.string,\n",
        "                                            name='examples')),\n",
        "  }\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "429-vvCWibO0"
      },
      "source": [
        "# Train the model using trainer component\n",
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(_taxi_trainer_module_file),\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cql1G35StJp"
      },
      "source": [
        "**Analyze Training**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXe62WE0S0Ek"
      },
      "source": [
        "model_artifact_directory = trainer.outputs['model'].get()[0].uri\n",
        "pp.pprint(os.listdir(model_artifact_dirrectory))\n",
        "model_dir = os.path.join(model_artifact_directory, 'serving_model_dir')\n",
        "pp.pprint(os.listdir(model_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-APzqz2NeAyj"
      },
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmPftrv0lEQy"
      },
      "source": [
        "**Evaluator** </br>\n",
        "The Evaluator component computes model performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVhfzzh9PDEx"
      },
      "source": [
        "eval_configuration = tfma.EvalConfig(\n",
        "    model_spec=[\n",
        "        # This assumes a serving model with signature 'serving_default'. \n",
        "        tfma.ModelSpec(label_key='tips')],\n",
        "    metrics_spec=[\n",
        "        tfma.MetricsSpec(\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.5}),\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': -1e-10})))\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "    slicing_spec=[\n",
        "        tfma.SlicingSpec(),\n",
        "        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjcx8g6mihSt"
      },
      "source": [
        "# Compute a evaluation statistics over features of a model and validate them.\n",
        "modelresolver = ResolverNode(\n",
        "      instance_name='latest_blessed_model_resolver',\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "      model=Channel(type=Model),\n",
        "      model_blessing=Channel(type=ModelBlessing))\n",
        "context.run(modelresolver)\n",
        "\n",
        "stats_evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "context.run(stats_evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4GghePOTJxL"
      },
      "source": [
        "stats_evaluator.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U729j5X5QQUQ"
      },
      "source": [
        "# Visualization of evalation output\r\n",
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyis6iy0HLdi"
      },
      "source": [
        "# Get the TFMA output result path and load the result.\n",
        "path_to_result = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_results = tfma.load_eval_result(path_to_result)\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_results, slicing_column='trip_start_hour')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxa5G08bSJ8a"
      },
      "source": [
        "path_result = evaluator.outputs['evaluation'].get()[0].uri\n",
        "print(tfma.load_validation_result(path_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DYekCZlHfj"
      },
      "source": [
        "**Pusher** </br>\n",
        "The Pusher component checks whether a model has passed validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r45nQ69eikc9"
      },
      "source": [
        "pusher_component = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher_component)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRkWo-MzTSss"
      },
      "source": [
        "# check the output of pusher component\r\n",
        "pusher_component.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyIqWl9TSdG"
      },
      "source": [
        "# If model was validated successfully, Pusher component will export the model in savedmodel format.\n",
        "pushuri = pusher.outputs.model_push.get()[0].uri\n",
        "model = tf.saved_model.load(pushuri)\n",
        "for i in model.signatures.items():\n",
        "  pp.pprint(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccwGfBRo3eqF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}