{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "Meta_Learning",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SruthiChilukuri/advanced_deep_learning/blob/master/Meta_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKfihic_71_3"
      },
      "source": [
        "##Meta Learning Implementation:\n",
        "\n",
        "Meta Learning means t that the network learns how to learn the the data.\n",
        "\n",
        "* The network's own learning process is learnt through the network  as it starts to train, from its own experience.\n",
        "* we will  use omniglot dataset to perform meta learning.\n",
        "\n",
        "* The data will be used to petform classification using a prototypical network.\n",
        "* The dataset contains handwritten digits and wwill use differemt exam[ples to classify  handwritten  digits from textual scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrpWuZtp68dy",
        "outputId": "601f5169-ab19-4fc0-c7ac-609b99e8917d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/ea/ab2c8c0e81bd051cc1180b104c75a865ab0fc66c89be992c4b20bbf6d624/tensorflow-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 40kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/ac/48dd71c2bdc8d31e367f9b72f25ccb3b89bc6b9d664fee21f9a8efa5714d/tensorboard-1.13.1-py2-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.13.1) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.13.1) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.13.1) (5.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (44.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.0\n",
            "    Uninstalling tensorflow-estimator-1.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.0\n",
            "  Found existing installation: tensorboard 2.1.0\n",
            "    Uninstalling tensorboard-2.1.0:\n",
            "      Successfully uninstalled tensorboard-2.1.0\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEyJvMJL6pO_"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9IXkOwY6pPD"
      },
      "source": [
        "### Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGKkMkLm6pPF",
        "outputId": "5ed8b264-4e13-4145-8537-0bc937075e15"
      },
      "source": [
        "Image.open('data/images/Japanese_(katakana)/character13/0608_01.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAqUlEQVR4nO3OMQ6CQBCF4X9wEyi3tOQYlhQeywKiByOehNJSuzUhjIXE8DDGimjBVPvlzeyMOZNqMqRWvvP0+zM8wqBpdTOz4kWPQA8EuJt8lb4uyt19wgQegIDhnCfNx+drGPcexiTKz70uSqXwqmlXCdta2GhzJvQgHAphj5AovJTCrhK2s9lauNF0r9zNZv+ebmYWNU2fmkvlFqAAAgC5S7rczSuX5gMYvSMsiQG6eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F93E0083210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XNkGSCKJ6pPL",
        "outputId": "d79e5f37-c087-421c-8aa6-803aef0df6a0"
      },
      "source": [
        "Image.open('data/images/Japanese_(katakana)/character13/0608_13.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAy0lEQVR4nO2SsQ3CQBAE594ICAhcAIFLoAKgNJsKKIFSEJVABwghYZDxEfhtWCMRIgIuG+3e/t3/m/NSRUDqj294+I0x9ooG3NR8UTwuBTeiei7qiTziGLin1por4BxtAWoo0tgzALf3mUcdbgErBy3OgKF36gQW5TMq8d2ceixRVaob6UFkgqWq66VgLqpHblUTtQ6CYarJmaBte1P9ALpi1jPnn3rTnnkFVwCad3SL8QGaz9L9nFhPHGqUVe1FN1GJv6qfN/rjt/EBTzonExbNmAgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F93E00A44D0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RKtytu6pPP",
        "outputId": "1f94f80b-9b1c-4b24-943b-b10b87b187da"
      },
      "source": [
        "Image.open('data/images/Sanskrit/character13/0863_09.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAAvUlEQVR4nO3TMQrCMBgF4PfHgB0cOjoI9ig5ikfwBrXgoRz1Bh6hR+jg0EJJHAT7XtGCi1gw25eXkP8niSXQqBxk/PkRKzNzmqa3i1/QBpYpXeI3ap4lu0zTLjDbyb3NMOkA1LVnnrcts9K9TshfxwFYTZ6rVVmvaVSWljMDpKoCPXN00GagB7BIR118UEr71NMjzYRxlOZKTq+4MU9oCmKPmtOIEIjJTIt0EHrmHmtmht2THljSJf3QA54b7wDYJdokvv1PAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F93E00A4410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvn1D97l6pPT",
        "outputId": "faf403a4-5e63-4700-a2bc-96e4f8879a23"
      },
      "source": [
        "Image.open('data/images/Sanskrit/character13/0863_13.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpAQAAAAAR+TCXAAAA+ElEQVR4nO3SsW2EMBTG8b+NFV2BBCMwQjaAkTJBRDa5TUI2yAiko3SkFBDBvRTmzHsprk2Ko0D6yebzZxsnqOfFY547/5Bvrk4MIB74dFVMo5c0MO+Tv+y375YdtBc4paiNKiJV3JMLsZMhp+dW9Y2SSzCczKg0NIrf0ClO0CuecxUPbH2po1ai5lyYGrE05NFwHAyd3YKwXbcQAFye4IEGRJWsgeHgCRgPBtwYj6hCWGbVCh4wdKuhOQ1bknO+FQ/wZE5S5lsbZDWdebYLSb94xW6YgmIzNjqq/ki/0c4yH7VPr0Iv5LheS0ocXg3bVif/6nzn/+IPc/48QDbAE1QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=1 size=105x105 at 0x7F94376D4050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHYwSVz6pPW"
      },
      "source": [
        "How can we convert this image into an array? we can use np.array function to convert these images into an array and then we reshape it to 28*28"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ0cRlrX6pPX"
      },
      "source": [
        "image_name = 'data/images/Sanskrit/character13/0863_13.png'\n",
        "alphabet, character, rotation = 'Sanskrit/character13/rot000'.split('/')\n",
        "rotation = float(rotation[3:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eheeEmnc6pPc",
        "outputId": "18a17d20-33a1-4aca-edbf-4b0150bc5283"
      },
      "source": [
        "np.array(Image.open(image_name).rotate(rotation).resize((28, 28)), np.float32,copy=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11mhhHCj6pPf"
      },
      "source": [
        "root_dir = 'data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF5_lhMQ6pPj"
      },
      "source": [
        "train_split_path = os.path.join(root_dir, 'splits', 'train.txt')\n",
        "\n",
        "with open(train_split_path, 'r') as train_split:\n",
        "    train_classes = [line.rstrip() for line in train_split.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4EDFI5R6pPm"
      },
      "source": [
        "#number of classes\n",
        "no_of_classes = len(train_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezAtrb-v6pPq"
      },
      "source": [
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "#image width\n",
        "img_width = 28\n",
        "\n",
        "#image height\n",
        "img_height = 28\n",
        "channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRN1OLT56pPv"
      },
      "source": [
        "train_dataset = np.zeros([no_of_classes, num_examples, img_height, img_width], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV_hso3K6pP1"
      },
      "source": [
        "for label, name in enumerate(train_classes):\n",
        "    alphabet, character, rotation = name.split('/')\n",
        "    rotation = float(rotation[3:])\n",
        "    img_dir = os.path.join(root_dir, 'data', alphabet, character)\n",
        "    img_files = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "  \n",
        "    \n",
        "    for index, img_file in enumerate(img_files):\n",
        "        values = 1. - np.array(Image.open(img_file).rotate(rotation).resize((img_width, img_height)), np.float32, copy=False)\n",
        "        train_dataset[label, index] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzzCST0E6pP3",
        "outputId": "a1a49b2f-fc19-4105-9214-f41e2a5a8ec8"
      },
      "source": [
        "train_dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4112, 20, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9W3ij26pP7"
      },
      "source": [
        "def convolution_block(inputs, out_channels, name='conv'):\n",
        "\n",
        "    conv = tf.layers.conv2d(inputs, out_channels, kernel_size=3, padding='SAME')\n",
        "    conv = tf.contrib.layers.batch_norm(conv, updates_collections=None, decay=0.99, scale=True, center=True)\n",
        "    conv = tf.nn.relu(conv)\n",
        "    conv = tf.contrib.layers.max_pool2d(conv, 2)\n",
        "    \n",
        "    return conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDpFeJ6o6pP_"
      },
      "source": [
        "def get_embeddings(support_set, h_dim, z_dim, reuse=False):\n",
        "\n",
        "        net = convolution_block(support_set, h_dim)\n",
        "        net = convolution_block(net, h_dim)\n",
        "        net = convolution_block(net, h_dim) \n",
        "        net = convolution_block(net, z_dim) \n",
        "        net = tf.contrib.layers.flatten(net)\n",
        "        \n",
        "        return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9auEriD26pQC"
      },
      "source": [
        "#number of classes\n",
        "num_way = 60  \n",
        "\n",
        "#number of examples per class for support set\n",
        "num_shot = 5  \n",
        "\n",
        "#number of query points\n",
        "num_query = 5 \n",
        "\n",
        "#number of examples\n",
        "num_examples = 20\n",
        "\n",
        "h_dim = 64\n",
        "\n",
        "z_dim = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QUPxyD6pQG"
      },
      "source": [
        "support_set = tf.placeholder(tf.float32, [None, None, img_height, img_width, channels])\n",
        "query_set = tf.placeholder(tf.float32, [None, None, img_height, img_width, channels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53YQeICH6pQJ"
      },
      "source": [
        "support_set_shape = tf.shape(support_set)\n",
        "query_set_shape = tf.shape(query_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYXgLPQ6pQN"
      },
      "source": [
        "num_classes, num_support_points = support_set_shape[0], support_set_shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S2Ueula6pQQ"
      },
      "source": [
        "num_query_points = query_set_shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SgZGtTJ6pQX"
      },
      "source": [
        "y = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "#convert the label to one hot\n",
        "y_one_hot = tf.one_hot(y, depth=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3t1nNsY6pQd"
      },
      "source": [
        "support_set_embeddings = get_embeddings(tf.reshape(support_set, [num_classes * num_support_points, img_height, img_width, channels]), h_dim, z_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnUTn-216pQg"
      },
      "source": [
        "embedding_dimension = tf.shape(support_set_embeddings)[-1]\n",
        "\n",
        "class_prototype = tf.reduce_mean(tf.reshape(support_set_embeddings, [num_classes, num_support_points, embedding_dimension]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KTI8MGC6pQo"
      },
      "source": [
        "query_set_embeddings = get_embeddings(tf.reshape(query_set, [num_classes * num_query_points, img_height, img_width, channels]), h_dim, z_dim, reuse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehM4Ydhd6pQt"
      },
      "source": [
        "def euclidean_distance(a, b):\n",
        "\n",
        "    N, D = tf.shape(a)[0], tf.shape(a)[1]\n",
        "    M = tf.shape(b)[0]\n",
        "    a = tf.tile(tf.expand_dims(a, axis=1), (1, M, 1))\n",
        "    b = tf.tile(tf.expand_dims(b, axis=0), (N, 1, 1))\n",
        "    return tf.reduce_mean(tf.square(a - b), axis=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfUl30ok6pQx"
      },
      "source": [
        "distance = euclidean_distance(class_prototype,query_set_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHKs6qPP6pQ1"
      },
      "source": [
        "predicted_probability = tf.reshape(tf.nn.log_softmax(-distance), [num_classes, num_query_points, -1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEV564yD6pQ6"
      },
      "source": [
        "loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_one_hot, predicted_probability), axis=-1), [-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW8dZLkJ6pQ_"
      },
      "source": [
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(predicted_probability, axis=-1), y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8exUyfQA6pRC"
      },
      "source": [
        "train = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bHvJHDL6pRF"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-m2YJMx6pRI"
      },
      "source": [
        "num_epochs = 20\n",
        "num_episodes = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Ntx6S0Vs6pRV",
        "outputId": "fc3fdd39-8a08-42c9-a771-a3c4b3ff1c4a"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        \n",
        "        # select 60 classes\n",
        "        episodic_classes = np.random.permutation(no_of_classes)[:num_way]\n",
        "        \n",
        "        support = np.zeros([num_way, num_shot, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        query = np.zeros([num_way, num_query, img_height, img_width], dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        for index, class_ in enumerate(episodic_classes):\n",
        "            selected = np.random.permutation(num_examples)[:num_shot + num_query]\n",
        "            support[index] = train_dataset[class_, selected[:num_shot]]\n",
        "            \n",
        "            # 5 querypoints per classs\n",
        "            query[index] = train_dataset[class_, selected[num_shot:]]\n",
        "            \n",
        "        support = np.expand_dims(support, axis=-1)\n",
        "        query = np.expand_dims(query, axis=-1)\n",
        "        labels = np.tile(np.arange(num_way)[:, np.newaxis], (1, num_query)).astype(np.uint8)\n",
        "        _, loss_, accuracy_ = sess.run([train, loss, accuracy], feed_dict={support_set: support, query_set: query, y:labels})\n",
        "        \n",
        "        if (episode+1) % 10 == 0:\n",
        "            print('Epoch {} : Episode {} : Loss: {}, Accuracy: {}'.format(epoch+1, episode+1, loss_, accuracy_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Episode 10 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n",
            "Epoch 1 : Episode 20 : Loss: 5.70378160477, Accuracy: 0.0166666675359\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}