{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_model_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R18QyJGAl4Ef"
      },
      "source": [
        "**Using BERT pretrained model and creating a question answer model using the BERT architecture.**\r\n",
        "\r\n",
        "The implementation has the following sequence of steps:\r\n",
        "\r\n",
        "* Setup BERT tekenizer\r\n",
        "* Loading the dataset and preprocessing steps\r\n",
        "* Create question answer model using BERT and then creating evalution(using a callback)\r\n",
        "* Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6OJQxUDmRRZ",
        "outputId": "f600f9e9-0728-414d-d64b-63a924f93589"
      },
      "source": [
        "! pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPTBxyDFmYO_",
        "outputId": "40d642d4-e28c-4a8c-d343-1318478cd3ab"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxTZbAFNl7dW"
      },
      "source": [
        "# Import required libraries\r\n",
        "import os\r\n",
        "import re\r\n",
        "import json\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvj-s2_UmLnh"
      },
      "source": [
        "# Define default parameters and configuration for BERT\r\n",
        "max_length = 384\r\n",
        "configuration = BertConfig()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68l_hN1imgJq"
      },
      "source": [
        "# Save the slow pretrained tokenizer\r\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n",
        "save_path = \"bert_base_uncased/\"\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "bert_tokenizer.save_pretrained(save_path)\r\n",
        "\r\n",
        "# Load the fast tokenizer from saved file\r\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ2MZJJdm50R"
      },
      "source": [
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\r\n",
        "val_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J_X-wZqnQ6Y",
        "outputId": "dc5b02bd-5e71-46b0-f09a-97a41e05b4e9"
      },
      "source": [
        "class SquadExample:\r\n",
        "    def __init__(self, question, context, start_char_index, answer_text, all_answer):\r\n",
        "        self.question = question\r\n",
        "        self.context = context\r\n",
        "        self.start_char_index = start_char_index\r\n",
        "        self.answer_text = answer_text\r\n",
        "        self.all_answer = all_answer\r\n",
        "        self.skip = False\r\n",
        "\r\n",
        "    def preprocess(self):\r\n",
        "        context = self.context\r\n",
        "        question = self.question\r\n",
        "        answer_text = self.answer_text\r\n",
        "        start_char_index = self.start_char_index\r\n",
        "\r\n",
        "        # Cleanning context, answer and questions\r\n",
        "        context = \" \".join(str(context).split())\r\n",
        "        question = \" \".join(str(question).split())\r\n",
        "        answer = \" \".join(str(answer_text).split())\r\n",
        "\r\n",
        "        # Finding end character index of answers in context\r\n",
        "        end_char_index = start_char_index + len(answer)\r\n",
        "        if end_char_index >= len(context):\r\n",
        "            self.skip = True\r\n",
        "            return\r\n",
        "\r\n",
        "        # Marking the character indexes in context of the answer\r\n",
        "        is_char_in_ans = [0] * len(context)\r\n",
        "        for i in range(start_char_index, end_char_index):\r\n",
        "            is_char_in_ans[i] = 1\r\n",
        "\r\n",
        "        # Tokenizing context\r\n",
        "        tokenized_context = tokenizer.encode(context)\r\n",
        "\r\n",
        "        # Finding tokens which were created from answer characters\r\n",
        "        ans_token_index = []\r\n",
        "        for i, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "                ans_token_index.append(i)\r\n",
        "\r\n",
        "        if len(ans_token_index) == 0:\r\n",
        "            self.skip = True\r\n",
        "            return\r\n",
        "\r\n",
        "        # Finding start and end token index for tokens from answer\r\n",
        "        start_token_index = ans_token_index[0]\r\n",
        "        end_token_index = ans_token_index[-1]\r\n",
        "\r\n",
        "        # Tokenize question\r\n",
        "        tokenized_question = tokenizer.encode(question)\r\n",
        "\r\n",
        "        # Create inputs\r\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\r\n",
        "            tokenized_question.ids[1:]\r\n",
        "        )\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        # Pad and create attention masks.\r\n",
        "        # Skip if truncation is needed\r\n",
        "        padding_length = max_length - len(input_ids)\r\n",
        "        if padding_length > 0:  # pad\r\n",
        "            input_ids = input_ids + ([0] * padding_length)\r\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "        elif padding_length < 0:  # skip\r\n",
        "            self.skip = True\r\n",
        "            return\r\n",
        "\r\n",
        "        self.input_ids = input_ids\r\n",
        "        self.token_type_ids = token_type_ids\r\n",
        "        self.attention_mask = attention_mask\r\n",
        "        self.start_token_index = start_token_index\r\n",
        "        self.end_token_index = end_token_index\r\n",
        "        self.context_token_to_char = tokenized_context.offsets\r\n",
        "\r\n",
        "\r\n",
        "with open(train_path) as f:\r\n",
        "    raw_train_data = json.load(f)\r\n",
        "\r\n",
        "with open(val_path) as f:\r\n",
        "    raw_val_data = json.load(f)\r\n",
        "\r\n",
        "\r\n",
        "def create_squad_example(raw_data):\r\n",
        "    squad_examples = []\r\n",
        "    for item in raw_data[\"data\"]:\r\n",
        "        for para in item[\"paragraphs\"]:\r\n",
        "            context = para[\"context\"]\r\n",
        "            for qa in para[\"qas\"]:\r\n",
        "                question = qa[\"question\"]\r\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\r\n",
        "                all_answer = [_[\"text\"] for _ in qa[\"answers\"]]\r\n",
        "                start_char_index = qa[\"answers\"][0][\"answer_start\"]\r\n",
        "                squad_eg = SquadExample(question, context, start_char_index, answer_text, all_answer)\r\n",
        "                squad_eg.preprocess()\r\n",
        "                squad_examples.append(squad_eg)\r\n",
        "    return squad_examples\r\n",
        "\r\n",
        "\r\n",
        "def create_inputs_target(squad_examples):\r\n",
        "    dataset_dict = {\r\n",
        "        \"input_ids\": [],\r\n",
        "        \"token_type_ids\": [],\r\n",
        "        \"attention_mask\": [],\r\n",
        "        \"start_token_index\": [],\r\n",
        "        \"end_token_index\": [],\r\n",
        "    }\r\n",
        "    for item in squad_examples:\r\n",
        "        if item.skip == False:\r\n",
        "            for key in dataset_dict:\r\n",
        "                dataset_dict[key].append(getattr(item, key))\r\n",
        "    for key in dataset_dict:\r\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\r\n",
        "\r\n",
        "    x = [\r\n",
        "        dataset_dict[\"input_ids\"],\r\n",
        "        dataset_dict[\"token_type_ids\"],\r\n",
        "        dataset_dict[\"attention_mask\"],\r\n",
        "    ]\r\n",
        "    y = [dataset_dict[\"start_token_index\"], dataset_dict[\"end_token_index\"]]\r\n",
        "    return x, y\r\n",
        "\r\n",
        "\r\n",
        "train_squad_example = create_squad_example(raw_train_data)\r\n",
        "x_train, y_train = create_inputs_target(train_squad_example)\r\n",
        "print(\"{} training points created.\".format(len(train_squad_example)))\r\n",
        "\r\n",
        "val_squad_example = create_squad_example(raw_val_data)\r\n",
        "x_val, y_val = create_inputs_target(val_squad_example)\r\n",
        "print(\"{} evaluation points created.\".format(len(val_squad_example)))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87599 training points created.\n",
            "10570 evaluation points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPtRNlvMskk6"
      },
      "source": [
        "def create_qa_model():\r\n",
        "    # BERT encoder\r\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\r\n",
        "\r\n",
        "    #Question-Answer Model\r\n",
        "    input_id = layers.Input(shape=(max_length,), dtype=tf.int32)\r\n",
        "    token_type_id = layers.Input(shape=(max_length,), dtype=tf.int32)\r\n",
        "    attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32)\r\n",
        "    embedding = encoder(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\r\n",
        "\r\n",
        "    start_logit = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\r\n",
        "    start_logit = layers.Flatten()(start_logit)\r\n",
        "\r\n",
        "    end_logit = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\r\n",
        "    end_logit = layers.Flatten()(end_logit)\r\n",
        "\r\n",
        "    start_prob = layers.Activation(keras.activations.softmax)(start_logit)\r\n",
        "    end_prob = layers.Activation(keras.activations.softmax)(end_logit)\r\n",
        "\r\n",
        "    model = keras.Model(\r\n",
        "        inputs=[input_id, token_type_id, attention_mask],\r\n",
        "        outputs=[start_prob, end_prob],\r\n",
        "    )\r\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\r\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\r\n",
        "    return model\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrKq4jeztxWq",
        "outputId": "884a7a1f-32df-4538-807d-5a16d73d4a13"
      },
      "source": [
        "use_tpu = True\r\n",
        "if use_tpu:\r\n",
        "    # Create distribution strategy\r\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "\r\n",
        "    # Create model\r\n",
        "    with strategy.scope():\r\n",
        "        model = create_qa_model()\r\n",
        "else:\r\n",
        "    model = create_qa_model()\r\n",
        "\r\n",
        "bert_model.summary()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.4.170.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.4.170.122:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.4.170.122:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.4.170.122:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 109482240   input_1[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       768         tf_bert_model_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       768         tf_bert_model_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,776\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KifkAaU3ur-Q"
      },
      "source": [
        "\r\n",
        "def text_normalization(txt):\r\n",
        "    txt = txt.lower()\r\n",
        "    # Remove punctuation\r\n",
        "    exclude = set(string.punctuation)\r\n",
        "    txt = \"\".join(ch for ch in txt if ch not in exclude)\r\n",
        "    # Remove article\r\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "    txt = re.sub(regex, \" \", txt)\r\n",
        "    # Remove extra white spaces\r\n",
        "    txt = \" \".join(txt.split())\r\n",
        "    return txt\r\n",
        "\r\n",
        "\r\n",
        "class ExactMatch(keras.callbacks.Callback):\r\n",
        "     def __init__(self, x_val, y_val):\r\n",
        "       self.x_val = x_val\r\n",
        "       self.y_val = y_val\r\n",
        "\r\n",
        "     def on_epoch_end(self, epoch, logs=None):\r\n",
        "       pred_start, pred_end = self.model.predict(self.x_val)\r\n",
        "       count = 0\r\n",
        "       val_examples_no_skip = [_ for _ in val_squad_example if _.skip == False]\r\n",
        "       for index, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "            squad_eg = val_examples_no_skip[index]\r\n",
        "            offset = squad_eg.context_token_to_char\r\n",
        "            start = np.argmax(start)\r\n",
        "            end = np.argmax(end)\r\n",
        "            if start >= len(offset):\r\n",
        "                continue\r\n",
        "            pred_char_start = offset[start][0]\r\n",
        "            if end < len(offset):\r\n",
        "                pred_char_end = offset[end][1]\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "            else:\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "\r\n",
        "            normalized_pred_ans = text_normalization(pred_ans)\r\n",
        "            normalized_true_ans = [text_normalization(_) for _ in squad_eg.all_answer]\r\n",
        "            if normalized_pred_ans in normalized_true_ans:\r\n",
        "                count += 1\r\n",
        "       acc = count / len(self.y_val[0])\r\n",
        "       print(\" \")\r\n",
        "       print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8vIFf8cwhOi",
        "outputId": "bea891a3-0b3b-499a-edbe-cbfd0e8ca6c4"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_val, y_val)\r\n",
        "model.fit(\r\n",
        "    x_train,\r\n",
        "    y_train,\r\n",
        "    epochs=2, \r\n",
        "    verbose=2,\r\n",
        "    batch_size=64,\r\n",
        "    callbacks=[exact_match_callback],\r\n",
        "    )\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.2070s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.2070s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "\n",
            "epoch=1, exact match score=0.77\n",
            "1346/1346 - 307s - loss: 0.5741 - activation_6_loss: 0.3117 - activation_7_loss: 0.2624\n",
            "Epoch 2/2\n",
            " \n",
            "\n",
            "epoch=2, exact match score=0.76\n",
            "1346/1346 - 307s - loss: 0.4365 - activation_6_loss: 0.2343 - activation_7_loss: 0.2021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7c7ca70198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfaVLF42xOFq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}